%!TEX root = ../ProgAlg.tex
\section{Shared Memory Systems}

\subsection{Shared-address-space platforms}
\begin{multicols}{2}
\begin{center}
    \includegraphics[width=0.5\linewidth]{images/UMA}
\end{center}
UMA (Uniform-memory-access)\\
shared-address-space computer with local caches and global memories \(\rightarrow\) all memory access times (except cache) are identical
\vfill\null
\columnbreak
\begin{center}
    \includegraphics[width=0.4\linewidth]{images/NUMA}
\end{center}
NUMA (Non-uniform-memory-access)\\
shared-address-space computer with local memory only \(\rightarrow\) local memory access times are shorter
\end{multicols}

\subsubsection{UMA Examples}
\begin{itemize}
    \item Intel Front Side Bus Architecture\\
    \begin{center}\includegraphics[width=0.6\linewidth]{images/umaExample}\end{center}
    \item Intel Pentium Front Side Bus Evolution\\
    \begin{center}\includegraphics[width=0.6\linewidth]{images/umaExample2}\end{center}
\end{itemize}

\subsubsection{NUMA Examples}
\begin{itemize}
    \item Example: Intel Core i7\\
    \begin{center}\includegraphics[width=0.45\linewidth]{images/numaExample}\end{center}
\end{itemize}

\subsection{Cache coherence}
\begin{itemize}
    \item Cache
    \begin{itemize}
        \item principle of space and time locality
        \item faster memory access
        \item additional hardware is required to keep multiple copies of data consistent with each other
    \end{itemize}
    \item Cache Coherence
    \begin{itemize}
        \item ensuring that concurrent operations on multiple copies of the same memory word have well-defined semantics
        \item this semantic is generally one of serializability
        \begin{itemize}
            \item there exists some serial order of instruction execution that corresponds to the parallel schedule
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsubsection{Update and Invalidate Protocols}
\begin{itemize}
    \item Scenario
    \begin{itemize}
        \item when a processor changes the value of its copy of a variable, one of two things must happen:
        \begin{itemize}
            \item the other copies must be invalidated (invalidate protocol) (write-back)\\
            \begin{center}\includegraphics[width=0.6\linewidth]{images/invalidateProtocol}\end{center}
            \item the other copies must be updated (update protocol) (write-through)\\
            \begin{center}\includegraphics[width=0.6\linewidth]{images/updateProtocol}\end{center}
        \end{itemize}
    \end{itemize}
    \item Pros and Cons
    \begin{itemize}
        \item if a processor just reads a value once and does not need it again, an update protocol may generate significant overhead
        \item if two processors make interleaved test and updates to a variable, an update protocol is better
        \item both protocols suffer from false sharing overheads (two words that are not shared, however, they lie on the same cache line)
    \end{itemize}
    \item Most current machines use invalidate protocols
    \begin{itemize}
        \item each copy of a data item is associated with a state (e.g. shared, invalid, or dirty)
        \begin{itemize}
            \item in shared state, there are multiple valid copies of the data item (therefore, an invalidate would have to be generated on an update)
            \item in dirty state, only one copy exists and therefore, no invalidates need to be generated
            \item in invalid state, the data copy is invalid, therefore, a read generates a data request (and associated state changes)
        \end{itemize}
    \end{itemize}
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/invalidateProtocolStateDiagram}
    \caption{State Diagram of an Invalidate Protocol}
\end{figure}

\subsection{Parallel programming in modern C++}
\begin{itemize}
    \item Explicit Threading
    \begin{itemize}
        \item data exchange is more apparent
        \begin{itemize}
            \item this helps in alleviating some of the overheads from data movement, false sharing, and contention
        \end{itemize}
        \item provides richer API in the form of condition waits, locks of different types, and increased flexibility for building composite synchronization operations
        \item tools and support are easier to find
    \end{itemize}
    \item Directives Layered on Top of Threads
    \begin{itemize}
        \item simplify a variety of thread-related tasks
        \item a programmer is rid of the tasks of initializing attributes objects, setting up arguments to threads, partitioning iteration spaces, etc.
    \end{itemize}
\end{itemize}

\subsubsection{Thread}
\begin{itemize}
    \item A thread is a single stream of control in the flow of a program. A program like\\
\begin{lstlisting}
for (row = 0; row < n; row++)
    for (column = 0; column < n; column++)
        c[row][column] = dot_product(get_row(a, row), get_col(b, col));
\end{lstlisting}
    \item can be transformed to\\
\begin{lstlisting}
for (row = 0; row < n; row++)
    for (column = 0; column < n; column++)
        thread([&]) {
            c[row][column] = dot_product(get_row(a, row), get_col(b, col));
        });
\end{lstlisting}
    \item In this case, one may think of the thread as an instance of a function that returns before the function has finished executing.
\end{itemize}

\subsection{Threads, Futures, Tasks}

\subsubsection{C++: Threads and futures}
\begin{itemize}
    \item Thread
    \begin{itemize}
        \item low-level
        \item exchange of data must be synchronized itself
        \item uncaught exceptions in the thread function lead to the termination of the entire program
        \item thread\_local storage class: static/global variables are created for each thread
        \item is started automatically in the constructor
    \end{itemize}
    \item Future
    \begin{itemize}
        \item asynchronous processing: parallel execution or when calling \lstinline{get()}
        \item exceptions appear in the parent thread when the result is picked up with \lstinline{get()}
        \item when leaving the scope of the responsible future, then its destructor ensures a clear and problem-free termination of the asynchronous computation
    \end{itemize}
\end{itemize}

\subsubsection{C++: Threads as the Basis of Parallelism}
\begin{itemize}
    \item Constructor and Executor
    \begin{itemize}
        \item \lstinline{thread(}executable object, parameters of the executable object\lstinline{)}
        \item Executable object
        \begin{itemize}
            \item function object (functor)
            \item lambda expression
            \item pointer to a function
        \end{itemize}
        \item the executable object and the parameters are copied by default, so the thread can work on their own data
    \end{itemize}
    \item Example\\
\begin{minipage}{0.4\linewidth}
\begin{lstlisting}
void printFibs(size_t from, size_t to);

struct Image {
    void fill(int r, int g, int b);
};
\end{lstlisting}
\end{minipage}%
\begin{minipage}{0.4\linewidth}
\begin{lstlisting}
int main() {
    thread th1(printFibs, 28, 35);
    Image img;
    thread th2([&img] { img.fill(0,1,2); });
    th1.join(); th2.join();
}
\end{lstlisting}
\end{minipage}
\end{itemize}

\subsubsection{C++: Futures and \lstinline{async}}
\begin{itemize}
    \item Asynchronous Computation
    \begin{itemize}
        \item parallel or deferred processing
        \item \lstinline{async()} initiates a computation and returns immediately
        \item \lstinline{get()} blocks until the result of the computation is available
    \end{itemize}
    \item Return value of \lstinline{async()}
    \begin{itemize}
        \item \lstinline{future<RT>}, where RT is the return type of the asynchronously executed function
    \end{itemize}
    \item Launch Policy
    \begin{itemize}
        \item \lstinline{async(launch::async, longComputation)} guarantees parallel execution (default)
        \item \lstinline{async(launch::deferred, computation)} executed when calling \lstinline{get()}
    \end{itemize}
    \item Behind the scenes
    \begin{itemize}
        \item a future can be produced without calling \lstinline{async()}, by first creating a promise (some kind of transmission channel)
    \end{itemize}
\end{itemize}

\paragraph{C++: Futures Example}~\\
\begin{lstlisting}
#include <future>

static size_t fibrec(size_t n) {
    return (n < 2) ? 1 : fibrec(n - 2) + fibrec(n - 1);
}

int main() {
    // asynchronous computation
    auto fut1 = async(launch::async, &fibrec, 35);
    // deferred computation
    auto fut2 = async(launch::deferred, &fibrec, 35);
    cout << fut2.get() << endl;    // waiting for the result of fut2
    cout << fut1.get() << endl;    // waiting for the result of fut1
}
\end{lstlisting}

What happens if the order of the two calls of \lstinline{get()} is changed?
\(\rightarrow\) \lstinline{fut2} would only start after fut1 would have finished \(\rightarrow\) serial execution

\subsubsection{C++: Packaged Tasks}
\begin{itemize}
    \item Concept
    \begin{itemize}
        \item a \lstinline{packaged_task} wraps a callable element and allows its result to be retrieved asynchronously
        \item it is similar to \lstinline{function}, but transferring its result automatically to a future object
    \end{itemize}
    \item Syntax
    \begin{itemize}
        \item \lstinline{template<class Ret, class... Args> class packaged_task<Ret(Args...)>;}
    \end{itemize}
    \item Object contains internally two elements
    \begin{itemize}
        \item a stored task, which is some callable object whose call signature shall take arguments of the types in \lstinline{Args...} and return a value of type \lstinline{Ret}
        \item a shared state, which is able to store the results of calling the stored task (of type \lstinline{Ret}) and be accessed asynchronously through a future
    \end{itemize}
\end{itemize}

\paragraph{C++: Packaged Tasks Example}~\\
\begin{lstlisting}
// create task for calling fibrec
// argument of fibrec has to be defined later
packaged_task<size_t(size_t)> task1(&fibrec);
auto f1 = task1.get_future(); // future for getting result

// create task for calling fibrec
// argument of fibrec is bound to 35
packaged_task<size_t(void)> task2(bind(&fibrec, 35));
auto f2 = task2.get_future(); // future for getting result

// call task1 in a parallel thread (move semantic)
thread th(move(task1), 35);
// call task2 in this thread
task2();

// get results
cout << f.get() << endl;
cout << f2.get() << endl;

th.join(); // this thread waits on parallel thread th
\end{lstlisting}

\subsubsection{C++: Synchronization Primitives}
\begin{itemize}
    \item Synchronized data access (read and write) is necessary if at least one of the parallel threads changes common data
    \item Synchronization Primitives
    \begin{itemize}
        \item{\makebox[2.5cm]{atomic\_xyz\hfill} all accesses are atomic (are not interrupted)}
        \item{\makebox[2.5cm]{atomic\_flag\hfill} atomic bool but lock-free}
        \item{\makebox[2.5cm]{once\_flag\hfill} used in call\_once, makes sure that only one of the parallel threads executes the function}
        \item{\makebox[2.5cm]{mutex\hfill} realizes mutual exclusion}
        \item{\makebox[2.5cm]{recursive\_mutex\hfill} allows a thread computing a recursive function to reenter a critical section}
        \item{\makebox[2.5cm]{lock\_guard\hfill} locks a critical section; very simple usage; the only state is locked}
        \item{\makebox[2.5cm]{unique\_lock\hfill} needs its unique mutex object; handles both states: locked and unlocked}
        \item{\makebox[2.5cm]{condition\_variable\hfill} blocks this thread until signaled (signals or notifications can be sent by other threads)}   
    \end{itemize}
\end{itemize}

\subsubsection{Serial for- vs. Parallel for-Loop}
\begin{itemize}
    \item Notice
    \begin{itemize}
        \item a lot of programming languages don't have a special parallel for-loop syntax
        \item they use the keyword \lstinline{for} in two situations
    \end{itemize}
    \item Serial for-Loop
    \begin{itemize}
        \item the loop notation simplifies the programming of a fixed number of repetitive, sequentially ordered steps
        \begin{itemize}
            \item Example: reading \(n\) integers from a sequential file
        \end{itemize}
    \end{itemize}
    \item Parallel for-Loop
    \begin{itemize}
        \item the loop notation simplifies the programming of a fixed number of repetitive steps, that can be done in any order
        \begin{itemize}
            \item Example: for each element of an array proceed the same task
        \end{itemize}
    \end{itemize}
\end{itemize}

\paragraph{C++: Possible parallel for-each implementation}~\\
\begin{lstlisting}
template<typename It>
void parallelForEach(It begin, It end, function<void(typename It::reference)> f) {
    const ptrdiff_t len = end - begin;
    if (len == 0) {
        return;
    } else if (len == 1) {
        f(*begin);
        return;
    }

    const It mid = begin + (ptrdiff_t)(len/2);
    future<void> fut = async(parallelForEach<It>, begin, mid, f);
    try {
        parallelForEach(mid, end, f);
    } catch (...) {
        fut.wait();
        throw;
    }
    fut.get();
}
\end{lstlisting}

\paragraph{C++: Usage of parallelForEach}~\\
\begin{lstlisting}
static size_t fibrec(size_t n) {
    return (n < 2) ? 1 : fibrec(n - 2) + fibrec(n - 1);
}

int main() {
    int vals[] = {5, 10, 15, 20, 25, 30, 35, 40};
    int size = sizeof(vals)/sizeof(int);

    parallelForEach(vals, vals + size, [](size_t n) {
        cout << fibrec(n) << endl;
    });
    return 0;
}
\end{lstlisting}

\subsubsection{Parallel Algorithms in C++17}
\begin{itemize}
    \item Ordering
    \begin{itemize}
        \item ''sequenced-before'' is an asymmetric, transitive, pair-wise relationship between evaluations within the same thread
        \item A is sequenced before B\\ \includegraphics[width=0.1\linewidth]{images/sequencedAB}
        \item if A is not sequenced before B and B is not sequenced before A, then two possibilities exist:
        \begin{itemize}
            \item evaluations of A and B are \textbf{indeterminately sequenced}: they may be performed in any order but may not overlap\\ \includegraphics[width=0.1\linewidth]{images/sequencedAB} or \includegraphics[width=0.1\linewidth]{images/sequencedBA}
            \item evaluations of A and B are \textbf{unsequenced}: they may be performed in any order and may overlap (within a single thread of execution, the compiler may interleave the CPU instructions that comprise A and B)\\ \includegraphics[width=0.1\linewidth]{images/sequencedBABA}
        \end{itemize}
    \end{itemize}
    \item Execution Policies
    \begin{itemize}
        \item most algorithms have overloads that accept execution policies:
        \item{\makebox[4.5cm]{\lstinline{std::execution::seq}\hfill} sequential execution like calling the algorithms without an execution policy}
        \item{\makebox[4.5cm]{\lstinline{std::execution::par}\hfill} execution potentially using multiple threads}
        \item[\-]{\makebox[4.5cm]{\hfill} parallel instructions are indeterminately sequenced}
        \item[\-]{\makebox[4.5cm]{\hfill} is not allowed to cause data races or to cause dead-locks}
        \item{\makebox[4.5cm]{\lstinline{std::execution::par_unseq}\hfill} execution may be parallelized, vectorized, or migrated across threads}
        \item[\-]{\makebox[4.5cm]{\hfill} parallel instructions and ordering in the same thread: unsequenced}
        \item[\-]{\makebox[4.5cm]{\hfill} use of blocking synchronization primitives (e.g. mutex) may cause dead-lock}
        \item{\makebox[4.5cm]{\lstinline{std::execution::unseq}\hfill} execution may be vectorized}
        \item[\-]{\makebox[4.5cm]{\hfill} ordering in the same thread: unsequenced}
    \end{itemize}
\end{itemize}

\paragraph{Execution Policies and Ordering Examples}~\\
\begin{lstlisting}
int a[] = {0, 1};
vector<int> v;
for_each(execution::par, begin(a), end(a), [&](int i) {
    v.push_back(i*2+1);  // Error: data race (vector isn't thread safe)
});

int x = 0;
mutex m;
for_each(execution::par, begin(a), end(a), [&](int) {
    lock_guard<mutex> guard(m);
    ++x;    // Correct, because mutual exclusion is guaranteed
});         // and sequence among parallel lambdas is irrelevant

for_each(execution::par_unseq, begin(a), end(a), [&](int) {
    lock_guard<mutex> guard(m);    // Error: calls m.lock() and several of these
    ++x;                           // calls are unsequenced and can interleave
})
\end{lstlisting}

\subsection{OpenMP}
\begin{itemize}
    \item A Standard for Directive Based Parallel Programming
    \begin{itemize}
        \item OpenMP is a directive-based API that can be used with
        \begin{itemize}
            \item FORTRAN
            \item C/C++
        \end{itemize}
        for programming shared address space machines
    \end{itemize}
    \item OpenMP directives provide support for
    \begin{itemize}
        \item concurrency
        \item synchronization
        \item data handling
    \end{itemize}
    while obviating the need for explicitly setting up
    \begin{itemize}
        \item mutexes
        \item condition variables
        \item data scope
        \item initialization
    \end{itemize}
    \item standard specifications: \url{http://www.openmp.org/specifications}
    \begin{itemize}
        \item Visual Studio 2019 only supports OpenMP standard 2.0
    \end{itemize}
\end{itemize}

\subsubsection{OpenMP Programming Model}
\begin{itemize}
    \item OpenMP in C/C++
    \begin{itemize}
        \item directives are based on the \lstinline{#pragma} compiler directives
        \item a directive consists of a directive name followed by clauses\\
        \lstinline{#pragma omp directive [clause list]}
    \end{itemize}
    \item OpenMP programs
    \begin{itemize}
        \item execute serially until they encounter the parallel directive, which creates a group of threads
\begin{lstlisting}
#pragma omp parallel [clause list]
/* structured block */
\end{lstlisting}
        \item the main thread that encounters the parallel directive becomes the \textbf{master} of this group of threads and is assigned the thread \textbf{id 0} within the group
        \item at the end of the parallel executed block the main thread waits for all parallel threads (\lstinline{join})
    \end{itemize}
\end{itemize}

\subsection{OpenMP: behind the scene}
\begin{center}
    \includegraphics[width=0.6\linewidth]{images/openMP}
\end{center}

\subsubsection{Clause List in OpenMP}
\begin{itemize}
    \item Clause List specifies
    \begin{itemize}
        \item Conditional Prallelization
        \begin{itemize}
            \item the clause \lstinline{if (scalar expr)} determines whether the parallel construct results in creation of threads
            \item the scalar expression is evaluated at runtim
        \end{itemize}
        \item Degree of Concurrency
        \begin{itemize}
            \item the clause \lstinline{num_threads(integer expr)} specifies the number of threads that are created
        \end{itemize}
        \item Data Handling
        \begin{itemize}
            \item the clause \lstinline{private (variable list)} indicates variables local to each thread T
            \item the clause \lstinline{firstprivate (variable list)} is similar to the \lstinline{private}, except values of variables are initialized to corresponding values before the parallel directive
            \item the clause \lstinline{shared (variable list)} indicates that variables are shared across all the threads
        \end{itemize}
    \end{itemize}
    \item Example
\begin{lstlisting}
#pragma omp parallel if(is_parallel == 1) num_threads(8) \
private(a) shared(b) firstprivate(c)
{
  /* structured block */
}
\end{lstlisting}
\end{itemize}

\subsubsection{Default Clause in OpenMP}
\begin{itemize}
    \item Syntax
    \begin{itemize}
        \item default(shared | none)
    \end{itemize}
    \item Semantic
    \begin{itemize}
        \item the default clause allows the user to affect the data-sharing attributes of variables
        \item omitting this clause is the same as the \lstinline{default(shared)}
    \end{itemize}
    \item default(shared)
    \begin{itemize}
        \item is equivalent to explicitly listing each currently visible variable in a \textbf{shared} clause, unless it is \textbf{threadprivate} or \textbf{const}-qualified
    \end{itemize}
    \item default(none)
    \begin{itemize}
        \item it is usually better style to use \lstinline{default(none)} instead of \lstinline{default(shared)}
        \item requires that at least one of the following must be true for every reference to a variable in the lexical extent of the parallel construct
        \begin{itemize}
            \item the variable is explicitly listed in a data-sharing attribute clause
            \item the variable is declared within the parallel construct
            \item the variable is \textbf{threadprivate} or has a \textbf{const}-qualified type
            \item the variable is the loop control variable for a \textbf{for}-loop that immediately follows a \textbf{for} or \textbf{parallel for}-directive, and the variable reference appears inside the loop
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsubsection{Default Clause in OpenMP}
\begin{itemize}
    \item Reduction Clause
    \begin{itemize}
        \item specifies how multiple local copies of a variable at different threads are combined into a single copy at the master when threads exit
        \item the usage is\\
        \lstinline{reduction (operator: variable list)}
        \item the variables in the list are implicitly specified as being private to threads
        \item the operator can be one of \lstinline{+, *, -, &, |, ^, &&, ||}
    \end{itemize}
    \item Example
\begin{lstlisting}
#pragma omp parallel default(none) reduction(+: sum) num_threads(8)
{
  /* compute local sums here */
  sum = ...;
}
/* sum here contains sum of all local instances of sums */
\end{lstlisting}
\end{itemize}

\subsubsection{OpenMP Programming: Example}
\begin{lstlisting}
int main() {
  const int npoints = 10000000;
  int sum = 0;
  mt19937_64 re; // random engine
  uniform_real_distribution<double> dist;
  #pragma omp parallel default(none) reduction(+: sum) num_threads(8)
  {
    #pragma omp for
    for (int i = 0; i < npoints; i++) {
      if (hypot(dist(re), dist(re)) < 1) sum++
    }
  }
  cout << setprecision(10) << 4.0*sum/npoints << endl;
}
\end{lstlisting}

\subsubsection{OpenMP Directives}
\begin{tabularx}{\linewidth}{|lX|}
    \hline
    Directive & Description\\
    \hline
    atomic & Specifies that a memory location will be updated atomically.\\
    barrier & Synchronizes all threads in a team; all threads pause at the barrier, until all threads execute the barrier.\\
    critical & Specifies that code is only executed on one thread at a time.\\
    flush & Specifies that all threads have the same view of memory for all shared objects.\\
    for & Causes the work done in a for-loop inside a parallel region to be divided among threads.\\
    master & Specifies that only the master thread should execute a section of the program.\\
    ordered & Specifies that code inside a parallelized for-loop should be executed by multiple threads in parallel.\\
    sections & Identifies code sections to be divided among all threads.\\
    single & Specifies that section of code should be executed on a single thread, not necessarily the master thread.\\
    threadprivate & Specifies that a variable is private to a thread.\\
    \hline
\end{tabularx}

\subsection{Concurrent tasks in OpenMP}
\begin{multicols}{2}
\begin{itemize}
    \item The \lstinline{for}-directive
    \begin{itemize}
        \item specifies concurrent iterations
        \item is used to split parallel iteration spaces across threads
        \item the general form is
\begin{lstlisting}
#pragma omp for [clause list]
/* for loop */
\end{lstlisting}
        \item allowed clauses
        \begin{itemize}
            \item \lstinline{private}
            \item \lstinline{firstprivate, lastprivate}
            \item \lstinline{reduction}
            \item \lstinline{schedule}
            \item \lstinline{nowait}
            \item \lstinline{ordered}
        \end{itemize}
    \end{itemize}
\end{itemize}
\vfill\null
\columnbreak
\begin{itemize}
    \item The \lstinline{sections}-directive
    \begin{itemize}
        \item specifies concurrent tasks
        \item the general form is
\begin{lstlisting}
#pragma omp sections [clause list]
{
  #pragma omp section
    /* structured block 1 */
  #pragma omp section
    /* structured block 2 */
  // ...
}
\end{lstlisting}
    \end{itemize}
\end{itemize}
\end{multicols}

\subsubsection{The \lstinline{for} Directive}
\begin{itemize}
    \item Schedule Clause
    \begin{itemize}
        \item deals with the assignment of iterations to threads
        \item the general form of the schedule directive is\\
        \lstinline{schedule(scheduling_class[, parameter])}
    \end{itemize}
    \item four scheduling classes
    \begin{itemize}
        \item static
        \begin{itemize}
            \item splits the iteration space into equal chunks of size \textit{parameter} and assigns them to threads in a round-robin fashion
            \item when no \textit{parameter} is specified, the iteration space is split into equally sized chunks, one chunk per thread
        \end{itemize}
        \item dynamic
        \begin{itemize}
            \item the iteration space is partitioned into chunks of size \textit{parameter} (default value: 1)
            \item these chunks are assigned to threads as they become idle
        \end{itemize}
        \item guided
        \begin{itemize}
            \item the chunk size is reduced exponentially as each chunk is dispatched to a thread
            \item the \textit{parameter} specifies the smallest chunk size (default value: 1)
        \end{itemize}
        \item runtime
        \begin{itemize}
            \item the environment variable OMP\_SCHEDULE determines at runtime the scheduling class and the chunk size
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsubsection{The \lstinline{schedule} Clause: Example}
\begin{lstlisting}
/* static scheduling of matrix multiplication loops */
#pragma omp parallel default(none) shared(a, b, c) num_thread(4)
  #pragma omp for schedule(static)
  for (int i = 0; i < 128; i++) {
    for (int j = 0; i < 128; j++) {
      c(i,j) = 0;
      for (int k = 0; k < 128; k++) {
        c(i,j) += a(i, k)*b(k, j);
      }
    }
  }
\end{lstlisting}

\subsubsection{The \lstinline{nowait} Clause}
\begin{itemize}
    \item Implicit Barrier
    \begin{itemize}
        \item at the end of the parallel \lstinline{for}-loop all threads join
    \end{itemize}
    \item Clause \lstinline{nowait}
    \begin{itemize}
        \item often, it is desirable to have a sequence of \lstinline{for}-directives within a parallel construct that do not execute an implicit barrier at the end of each \lstinline{for} directive
    \end{itemize}
    \item Example
\begin{lstlisting}
#pragma
{
  #pragma omp for nowait
  for (int i = 0; i < nmax; i++)
    if (isEqual(name, current_list[i])) processCurrName(name);
  #pragma omp for
  for (int i = 0; i < nmax; i++)
    if (isEqual(name, past_list[i])) processPastName(name);
}
\end{lstlisting}
\end{itemize}

\subsubsection{The \lstinline{sections} Directive}
\begin{itemize}
    \item Sections
    \begin{itemize}
        \item supports non-iterative parallel task assignment using the \lstinline{sections} directive
    \end{itemize}
    \item Example
\begin{lstlisting}
#pragma omp parallel
{
  #pragma omp sections
  {
    #pragma omp section
    {
      taskA();
    }
    #pragma omp section
    {
      taskB();
    }
    #pragma omp section
    {
      taskC();
    }
  }
}
\end{lstlisting}
\end{itemize}

\subsubsection{Merging Directives}
\begin{itemize}
    \item Remember
    \begin{itemize}
        \item the \textbf{parallel} directive creates the group of threads
        \item the \lstinline{for} and the \lstinline{sections} directive would execute serially (by the master thread) if no \lstinline{parallel} directive is specified before
    \end{itemize}
    \item Merging \lstinline{parallel} and \lstinline{for} directives
\begin{multicols}{2}
\begin{lstlisting}
#pragma omp parallel shared(n)
{
  #pragma omp for
  for (int i = 0; i < n; i++) {
      // ...
  }
}
\end{lstlisting}
\vfill\null
\columnbreak
\begin{lstlisting}
#pragma omp parallel for shared(n)
for (int i = 0; i < n; i++) {
  // ...
}
\end{lstlisting}
\end{multicols}
\end{itemize}

\subsubsection{Nesting \lstinline{parallel} Directives}
\begin{itemize}
    \item Example
\begin{lstlisting}
#pragma omp parallel for shared(a, b, c) num_threads(4)
for (int i = 0; i < 128; i++) {
  #pragma omp parallel for shared(a, b, c) num_threads(4)
  for (int j = 0; j < 128; j++) {
    c(i,j) = 0;
    #pragma omp parallel for shared(a, b, c) num_threads(4)
    for (int k = 0; k < 128; k++) {
      c(i,j) += a(i, k)*b(k, j);
    }
  }
}
\end{lstlisting}
    \item Remarks
    \begin{itemize}
        \item OpenMP does not allow nested \lstinline{for}, \lstinline{sections}, and \lstinline{single} directives that bind to the same parallel directive
        \item each \lstinline{for} directive brings its own \lstinline{parallel} directive, which only generates a logical team of threads on encountering a nested \lstinline{parallel} directive
        \item the newly generated logical team is still executed by the same thread corresponding to the outer \lstinline{parallel} directive
        \item to generate a new set of threads, nested parallelism must be enabled by setting the OMP\_NESTED environment variable to TRUE
    \end{itemize}
\end{itemize}

\subsection{Synchronization in OpenMP}
\begin{itemize}
    \item Synchronization Constructs
    \begin{itemize}
        \item \lstinline{#pragma omp barrier}
        \item \lstinline{#pragma omp single [clause list]}\\
        \lstinline{/* structured block */}
        \item \lstinline{#pragma omp master}\\
        \lstinline{/* structured block */}
        \item \lstinline{#pragma omp critical [(name)]}\\
        \lstinline{/* structured block */}
        \item \lstinline{#pragma omp atomic}\\
        \lstinline{/* memory update instruction */}
        \item \lstinline{#pragma omp ordered}\\
        \lstinline{/* structured block */}
        \item \lstinline{#pragma omp flus [(variable list)]}
    \end{itemize}
\end{itemize}

\subsubsection{Example: Prefix Sums}
\begin{lstlisting}
cumulSum[0] = list[0];
#pragma omp parallel for default(none) shared(cumulSum, list) ordered

for (int i = 1; i < n; i++) {
  // other work
  #pragma omp ordered
  {
    cumulSum[i] = cumulSum[i - 1] + list[i];
  }
}
\end{lstlisting}

\subsubsection{Data Handling in OpenMP}
\begin{itemize}
    \item Which data class should you use when?
    \begin{itemize}
        \item \lstinline{private}
        \begin{itemize}
            \item a thread initializes and uses a variable and no other thread accesses the data
            \item it is better to use a local variable in an omp block
            \item if multiple threads manipulate different parts of a large data structure, the programmer should explore ways of breaking it into smaller data structures and making them \lstinline{private} to the thread that manipulates them
        \end{itemize}
        \item \lstinline{firstprivate}
        \begin{itemize}
            \item a thread repeatedly reads a variable that has been initialized earlier in the program
        \end{itemize}
        \item \lstinline{reduction}
        \begin{itemize}
            \item if multiple threads manipulate a single piece of data, one must explore ways of breaking these manipulations into local operations followed by a single global operation (reduction)
        \end{itemize}
        \item \lstinline{threadprivate(variable list)}
        \begin{itemize}
            \item all variables in the list are local to each thread and are initialized once before they are accessed in a parallel region
            \item these variables persist across different parallel regions provided dynamic adjustment of the number of threads is disabled and the number of threads is the same
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{OpenMP library functions}
\begin{itemize}
    \item Thread and Processor Count
    \begin{itemize}
        \item \lstinline{#include <omp.h}
        \item \lstinline{void omp_set_num_threads (int num_threads);}
        \item \lstinline{int omp_get_num_threads();}
        \item \lstinline{int omp_get_max_threads();}
        \item \lstinline{int omp_get_thread_num();}
        \item \lstinline{int omp_get_num_procs();}
        \item \lstinline{int omp_in_parallel();}
    \end{itemize}
    \item Controlling and Monitoring Thread Creation
    \begin{itemize}
        \item \lstinline{void omp_set_dynamic(int dynamic);}
        \item \lstinline{int omp_get_dynamic();}
        \item \lstinline{void omp_set_nested(int nested);}
        \item \lstinline{int omp_get_nested();}
    \end{itemize}
    \item Mutual Exclusion
    \begin{itemize}
        \item \lstinline{void omp_init_lock(omp_lock_t *lock);}
        \item \lstinline{void omp_destroy_lock(omp_lock_t *lock);}
        \item \lstinline{void omp_set_lock(omp_lock_t *lock);}
        \item \lstinline{void omp_unset_lock(omp_lock_t *lock);}
        \item \lstinline{int omp_test_lock(omp_lock_t *lock);}
    \end{itemize}
\end{itemize}

\subsubsection{Environment Variables in OpenMP}
\begin{itemize}
    \item Environment Variables
    \begin{itemize}
        \item OMP\_NUM\_Threads
        \begin{itemize}
            \item specifies the default number of threads created upon entering a parallel region
        \end{itemize}
        \item OMP\_SET\_DYNAMIC
        \begin{itemize}
            \item determines if the number of threads can be dynamically changed
        \end{itemize}
        \item OMP\_NESTED
        \begin{itemize}
            \item turns on nested parallelism
        \end{itemize}
        \item OMP\_SCHEDULE
        \begin{itemize}
            \item scheduling of \lstinline{for}-loops if the clause specifies runtime
        \end{itemize}
    \end{itemize}
    \item Common Mistakes in OpenMP programs
    \begin{itemize}
        \item \url{http://michaelsuess.net/publications/suess_leopold_common_mistakes_06.pdf}
    \end{itemize}
\end{itemize}

\subsubsection{OpenMP Standard}
\begin{itemize}
    \item Version 3.1
    \begin{itemize}
        \item major change since version 2.5
        \begin{itemize}
            \item tasks added to execution model
        \end{itemize}
    \end{itemize}
    \item Version 4.0
    \begin{itemize}
        \item major changes since version 3.1
        \begin{itemize}
            \item array syntax extended to support array sections
            \item \lstinline{proc_bind} clause to support thread affinity policies
            \item SIMD contructs to support SIMD parallelism
            \item device constructs to support execution of devices (e.g. GPU)
            \item user defined reductions
            \item \lstinline{depend} clause to support task dependencies
        \end{itemize}
    \end{itemize}
    \item Version 5.1
    \begin{itemize}
        \item major changes since version 4.5
        \begin{itemize}
            \item extended memory model to distinguish different types of flush operations
            \item support of modern C++20
        \end{itemize}
    \end{itemize}
\end{itemize}